{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG64IayhZt8I"
      },
      "source": [
        "## 2.0 Analysis of 20 NewsGroups data\n",
        "\n",
        "This is the finalised version  the Analysis section. Included are:\n",
        "\n",
        "\n",
        "1) Summary Statistics of Data\n",
        "\n",
        "2) Most common words for each category\n",
        "\n",
        "3) Analysis of Parts of Speech \n",
        "\n",
        "4) Bigram analysis\n",
        "\n",
        "\n",
        "data imported as csv after text cleaning, but pre-vectorising.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlUmqVv1Ztl2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYgvapkBPFjU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmh0sqEqOxw3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "318BBdQCO2MH"
      },
      "outputs": [],
      "source": [
        "proj_dir='/content/drive/MyDrive/Colab Notebooks/doc2vec/' #give your project directory here. data sets should be in this location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZhGOcqlco7e"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA58FnnpO5s6"
      },
      "outputs": [],
      "source": [
        "''' loading the cleaned data '''\n",
        "\n",
        "train_cleaned_df = pd.read_csv(proj_dir+'train_cleaned_data.csv')\n",
        "train_cleaned_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Summary statistics of data"
      ],
      "metadata": {
        "id": "MVklm1tkUN4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoYJykFIRtUG"
      },
      "outputs": [],
      "source": [
        "# number of documents\n",
        "print(len(train_cleaned_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3SLPSOpR3Mt"
      },
      "outputs": [],
      "source": [
        "# number documents in each category\n",
        "print(train_cleaned_df[\"folder name\"].value_counts())\n",
        "\n",
        "# output as a pandas dataframe\n",
        "data_summary_df = pd.DataFrame(train_cleaned_df[\"folder name\"].value_counts()).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_summary_df"
      ],
      "metadata": {
        "id": "T9I2r39obadS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NTESAGVVvtx"
      },
      "outputs": [],
      "source": [
        "# Fix NaNs\n",
        "train_cleaned_df['text cleaned'] = np.where(train_cleaned_df['text cleaned'].isnull(),train_cleaned_df['folder name'],train_cleaned_df['text cleaned'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YjkBJt1catS"
      },
      "outputs": [],
      "source": [
        "# count words per category\n",
        "(train_cleaned_df.assign(text=train_cleaned_df['text cleaned'].str.split()).explode(\"text cleaned\")\n",
        " .groupby(\"folder name\",sort=False)['text cleaned'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN_opJFQSQin"
      },
      "outputs": [],
      "source": [
        "# Group Data by category\n",
        "train_cl_gr_df   = train_cleaned_df.groupby('folder name').agg({'text cleaned': ','.join}).reset_index()\n",
        "train_cl_gr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMTW_18ocYsH"
      },
      "outputs": [],
      "source": [
        "# total words per group\n",
        "train_cl_gr_df['total words'] =train_cl_gr_df['text cleaned'].apply(lambda x: len(x.split()))\n",
        "\n",
        "data_summary_df['total words'] = train_cl_gr_df['text cleaned'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# average words per doc\n",
        "\n",
        "data_summary_df['average words'] = data_summary_df['total words'] / data_summary_df['folder name']\n",
        "\n",
        "data_summary_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mk9oGEqjby6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Most common words in each category"
      ],
      "metadata": {
        "id": "_BctjeFiUDE2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu-UFDrfWMg7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "Plots of  most common words by category\n",
        "\n",
        "'''\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)  \n",
        "for folder in train_cl_gr_df['folder name']:\n",
        "    target_df = train_cl_gr_df[(train_cl_gr_df['folder name'] == folder)]\n",
        "\n",
        "    print (\"target_df: \", len(target_df))\n",
        "    target_df.head(10)\n",
        "    string_text =  target_df['text cleaned'].to_string()\n",
        "\n",
        "    string_text=string_text.replace('\\n',' ')\n",
        "    string_text = re.sub(r\"[^A-Za-z0-9 ]+\", \"\", string_text)\n",
        "\n",
        "    print(string_text)\n",
        "\n",
        "\n",
        "    vocab ={}\n",
        "    for word in string_text.split():\n",
        "          if len(word) != 1: \n",
        "                if vocab.get(word.lower()) != None:\n",
        "                    vocab[word.lower()] += 1\n",
        "\n",
        "                    ##If word is not in dictionary then we put that word in our dictinary by making its frequnecy 1\n",
        "                else:\n",
        "                    vocab[word.lower()] = 1\n",
        "                \n",
        "    d = Counter(vocab)\n",
        "\n",
        "    most_common = d.most_common(10)\n",
        "\n",
        "    print('Most Common Words: ' , folder  , d)\n",
        "    \n",
        "    words = [word for word, _ in most_common]\n",
        "    counts = [counts for _, counts in most_common]\n",
        "\n",
        "    plt.bar(words, counts)\n",
        "    plt.title(\"10 most Common Words Category: \"  + str(folder))\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.rcParams['figure.figsize'] = [15, 5]\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofTioofidv7k"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Part of Speech Analysis\n",
        "\n",
        "Identify number of nouns and verbs etc in each group and display as a stacked bar chart"
      ],
      "metadata": {
        "id": "s1JrusRLT_8n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zPcH0Rsdv5X"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def NounCount(x):\n",
        "    nounCount = sum(1 for word, pos in pos_tag(word_tokenize(x)) if pos.startswith('NN'))\n",
        "    return nounCount\n",
        "\n",
        "train_cl_gr_df[\"nouns\"] = train_cl_gr_df[\"text cleaned\"].apply(NounCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjScILruqdni"
      },
      "outputs": [],
      "source": [
        "def VerbCount(x):\n",
        "  \n",
        "    verbCount = sum(1 for word, pos in pos_tag(word_tokenize(x)) if pos.startswith('VB'))\n",
        "    return  verbCount\n",
        "\n",
        "train_cl_gr_df[\"verbs\"] = train_cl_gr_df[\"text cleaned\"].apply(VerbCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfZFcfc6h9yV"
      },
      "outputs": [],
      "source": [
        "# classify adverbs\n",
        "def AdverbCount(x):\n",
        "  \n",
        "    adverbCount = sum(1 for word, pos in pos_tag(word_tokenize(x)) if pos.startswith('RB'))\n",
        "    return  adverbCount\n",
        "\n",
        "train_cl_gr_df[\"adverbs\"] = train_cl_gr_df[\"text cleaned\"].apply(AdverbCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAFWgeCNilTP"
      },
      "outputs": [],
      "source": [
        "# classify adjectives\n",
        "def AdjectiveCount(x):\n",
        "  \n",
        "    adjectiveCount = sum(1 for word, pos in pos_tag(word_tokenize(x)) if pos.startswith('JJ'))\n",
        "    return  adjectiveCount\n",
        "\n",
        "train_cl_gr_df[\"adjectives\"] = train_cl_gr_df[\"text cleaned\"].apply(AdjectiveCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go4qMAiQkqhg"
      },
      "outputs": [],
      "source": [
        "# classify other\n",
        "def OtherCount(x):\n",
        "  \n",
        "    otherCount = sum(1 for word, pos in pos_tag(word_tokenize(x)) if not pos.startswith(('NN', 'VB', 'RB','JJ')))\n",
        "    return  otherCount\n",
        "\n",
        "train_cl_gr_df[\"other\"] = train_cl_gr_df[\"text cleaned\"].apply(OtherCount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHlZ0Fpk2n--"
      },
      "outputs": [],
      "source": [
        "train_cl_gr_df[[\"folder name\", \"nouns\",\"verbs\", \"adverbs\", \"adjectives\", \"other\" ]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualisation to show split of word types\n",
        "\n",
        "ax = train_cl_gr_df.plot.barh(stacked=True,  title='POS Categorisation', x='folder name')"
      ],
      "metadata": {
        "id": "pZL3epIL7mY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETlN837O2wBm"
      },
      "outputs": [],
      "source": [
        "'''Scaled - proportions of POS\n",
        "Show the POS values as the proportion of the total number of words for each category\n",
        "\n",
        "'''\n",
        "df = train_cl_gr_df[[ \"nouns\",\"verbs\", \"adverbs\", \"adjectives\", \"other\" ]]\n",
        "df.loc[:,'Row_Total'] = df.sum(numeric_only=True, axis=1)\n",
        "\n",
        "df['nouns'] = 100* (df['nouns'] / df['Row_Total'] )\n",
        "df['verbs'] = 100* (df['verbs'] / df['Row_Total'] )\n",
        "df['adverbs'] = 100* (df['adverbs'] / df['Row_Total'] )\n",
        "df['adjectives'] = 100* (df['adjectives'] / df['Row_Total'] )\n",
        "df['other'] = 100* (df['other'] / df['Row_Total'] )\n",
        "df = df[[ \"nouns\",\"verbs\", \"adverbs\", \"adjectives\", \"other\" ]]\n",
        "\n",
        "df2 = train_cl_gr_df[['folder name']]\n",
        "\n",
        "train_pos_df_scaled =df2.join(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = train_pos_df_scaled.plot.barh(stacked=True,  title='POS Categorisation (Scaled)', x='folder name')"
      ],
      "metadata": {
        "id": "24TGmWAX34Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bigrams\n",
        "\n",
        "This section calculates the most frequently occuring bigrams"
      ],
      "metadata": {
        "id": "J5FdefF8EajN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "fd = FreqDist(train_cl_gr_df[['text cleaned']].to_string().split())\n",
        "fd.plot(20)"
      ],
      "metadata": {
        "id": "ibFbvjVyEZ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#frequency of bigrams\n",
        "from nltk import bigrams\n",
        "import matplotlib.pyplot as plt\n",
        "#fig = plt.figure(figsize = (10,4))\n",
        "fd_bg = FreqDist(map(' '.join, bigrams(train_cl_gr_df[['text cleaned']].to_string().split())))\n",
        "fd_bg.plot(50, title='Top 30 Most Common Bigrams in Whole Text')\n",
        "\n",
        "fig.savefig('freqDist.png', bbox_inches = \"tight\")"
      ],
      "metadata": {
        "id": "ePf1c0rgKLvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vxFm5tCGEQUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZK-8C1Yd-mk"
      },
      "outputs": [],
      "source": [
        "# Bigrams :  pointwise mutual information\n",
        "\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "\n",
        "BigramCollocationFinder.from_words(train_cl_gr_df[['text cleaned']].to_string().split()).\\\n",
        "    nbest(BigramAssocMeasures().pmi, 20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BigramCollocationFinder.from_words(train_cl_gr_df[['text cleaned']].to_string().split()).\\\n",
        "    score_ngrams(BigramAssocMeasures().pmi)"
      ],
      "metadata": {
        "id": "1OPg5zvd_wdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  20.39 --> there isn't any difference in significance between these first few hundred bigrams."
      ],
      "metadata": {
        "id": "pfmyJ094_wVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
        "TrigramCollocationFinder.from_words(train_cl_gr_df[['text cleaned']].to_string().split()).\\\n",
        "    nbest(TrigramAssocMeasures().pmi, 10)"
      ],
      "metadata": {
        "id": "vH5rB8ojBdc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8i5UyzkN_wSc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2.0 Analysis of 20 NewsGroups Data .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}