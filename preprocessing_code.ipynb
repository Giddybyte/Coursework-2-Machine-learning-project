{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f6875224",
        "b0c3a105"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "074718ea"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe589fc",
        "outputId": "1d498430-bd61-4764-8302-4d82d6bc95bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk and gensim are two famous libraries that are used in Natural Language Processing (NLP). nltk library has been used to get the stop words of English language and to lemmatize words. Also gensim library has been used to load the Doc2vec model"
      ],
      "metadata": {
        "id": "StcGasMZuwXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0RpfabEdewH",
        "outputId": "e4158d39-52e6-4b9c-c5d6-f71d1c92f35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proj_dir='/content/drive/MyDrive/Colab Notebooks/doc2vec/' #give your project directory here. data sets should be in this location"
      ],
      "metadata": {
        "id": "Q6JZ4NVKdjRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6875224"
      },
      "source": [
        "# Load and clean the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7345380"
      },
      "source": [
        "before converting into vectors text should be cleaned.\n",
        "\n",
        "* Lower the english latters\n",
        "* Remove headers\n",
        "* Drop all digits\n",
        "* Remove URLs and Emails\n",
        "* Drop all punctuation from our text\n",
        "* Drop stop words\n",
        "* Lemmatize words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "first, we convert all the letters into lowercase letters in order to avoid duplicating the same word ('Dog' and 'dog' are the same words. but the program will take these as two words. Hence we convert 'Dog' as 'dog' by doing letters lowercase)"
      ],
      "metadata": {
        "id": "cU1xKL982Jt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that we drop all the headers and numbers.Because headers are unwanted data and numbers don't have any meaning when we take them as a single word."
      ],
      "metadata": {
        "id": "VCSbcZAx2J0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if any word contains '@','.com' or 'http' we drop those words. also we remove all punctuations and stop words. The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words such as “the”, “a”, “an”, “so”, “what” in English language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information."
      ],
      "metadata": {
        "id": "MkQTCdcm2Q1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "finally we lemmatize words. Lemmatization is the process wherein the context is used to convert a word to its meaningful base or root form. ex:- driving--> drive , dogs-->dog. To perform Lemmatization with Natural Language Tool Kit (NLTK), “WordNetLemmatizer()” method has been used"
      ],
      "metadata": {
        "id": "X29HHeUe2WZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e109337d"
      },
      "outputs": [],
      "source": [
        "'''this function is used to read text files.'''\n",
        "def read_txt_file(file_name):\n",
        "    with open(file_name,encoding=\"utf8\", errors='ignore') as f:\n",
        "        ###extract the boady of the text###\n",
        "        line = f.readline()\n",
        "        txt=''\n",
        "        txt=txt+' '+line\n",
        "        while line:\n",
        "            line = f.readline()\n",
        "            txt=txt+' '+line\n",
        "        ###################################\n",
        "    f.close()\n",
        "    return(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19a41917"
      },
      "outputs": [],
      "source": [
        "stop_words=set(stopwords.words('english'))#load stop words\n",
        "punctuations=string.punctuation #get punctuations\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "'''this function is used to clean text'''\n",
        "def clean_txt(txt):\n",
        "    txt=txt.lower() #set all characters to lowercase\n",
        "    sentences=txt.split('\\n')\n",
        "    txt = ' '.join([i for i in sentences if not ':' in i])#remove headers\n",
        "    txt = ''.join([i for i in txt if not i.isdigit()])#remove numbers\n",
        "\n",
        "    ###remove urls and emails###\n",
        "    words=txt.split()\n",
        "    txt = ' '.join([i for i in words if not '@' in i and not '.com' in i and not  'http:' in i])\n",
        "    #######################################\n",
        "\n",
        "    ###remove punctuations###\n",
        "    for character in punctuations:\n",
        "        txt = txt.replace(character, '')\n",
        "    #########################################\n",
        "    \n",
        "    ###remove stop words and lemmatize###\n",
        "    words=txt.split()\n",
        "    filtered_txt = ' '.join([lemmatizer.lemmatize(i) for i in words if not i in stop_words])\n",
        "    #####################################\n",
        "    \n",
        "    return(filtered_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95c8ac31"
      },
      "outputs": [],
      "source": [
        "def load_and_clean_data(location):    \n",
        "    y=os.listdir(location)#get the list of folder\n",
        "    txts=[]\n",
        "    txts_cleaned=[]\n",
        "    folder_array=[]\n",
        "    file_array=[]\n",
        "    for i in range(len(y)):\n",
        "        text_file_names=os.listdir(location+'/'+y[i]) #get the list of files\n",
        "        for text_file_name in text_file_names:\n",
        "                file_array.append(text_file_name)\n",
        "                txt=read_txt_file(location+'/'+y[i]+'/'+text_file_name) #read the text file\n",
        "                txts.append(txt)\n",
        "                txts_cleaned.append(clean_txt(txt)) #clean the text\n",
        "                folder_array.append(y[i])\n",
        "\n",
        "    ###create a data frame###\n",
        "    df=pd.DataFrame()\n",
        "    df['texts']=txts\n",
        "    df['text cleaned']=txts_cleaned\n",
        "    df['folder name']=folder_array\n",
        "    df['file name']=file_array\n",
        "    ########################\n",
        "    return (df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "179438fd"
      },
      "outputs": [],
      "source": [
        "df_train=load_and_clean_data(proj_dir+'20news-bydate-train')\n",
        "df_test=load_and_clean_data(proj_dir+'20news-bydate-test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfa1b61d"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84a69f31"
      },
      "outputs": [],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0c3a105"
      },
      "source": [
        "# Convert to vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f974967b"
      },
      "source": [
        "document is tokenized into words and applied doc2vec model. doc2vec is a method to represent list of words using a vector. it is used to create a vectorised representation of a group of words taken collectively as a single unit.In gensim the model will always be trained on a word per word basis. Therefore  we split the document into an array of words using split(). In order to train the model, tagged documents are needed. it can be created by using models.doc2vec.TaggedDcument(). then finally we train the doc2vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56b11ddb"
      },
      "outputs": [],
      "source": [
        "'''this function is used to do tokenization'''\n",
        "def tokenizer(txt):  \n",
        "    tokens=txt.split(' ')\n",
        "    unique_tokens=np.unique(np.array(tokens)) #get unique tokens\n",
        "    ###create a dictonary of tokens###\n",
        "    tokens_dict={}\n",
        "    for indx in range(len(unique_tokens)):\n",
        "        tokens_dict[unique_tokens[indx]]=indx\n",
        "    return(tokens_dict,tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5543b28"
      },
      "outputs": [],
      "source": [
        "#tokanize train and test data\n",
        "words_list=[]\n",
        "for i in range(len(df_train)):\n",
        "    _,words=tokenizer(df_train['text cleaned'][i]) \n",
        "    words_list.append(words)\n",
        "for i in range(len(df_test)):\n",
        "    _,words=tokenizer(df_test['text cleaned'][i])\n",
        "    words_list.append(words)\n",
        "####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8f42138"
      },
      "outputs": [],
      "source": [
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(words_list)]\n",
        "model = Doc2Vec(vector_size=40, min_count=2, epochs=30) #fit the Doc2Vec model\n",
        "model.build_vocab(documents)\n",
        "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ab99bf3"
      },
      "outputs": [],
      "source": [
        "###get train vectors##\n",
        "train_vectors=[]\n",
        "for i in range(len(df_train)):\n",
        "    train_vectors.append(model.infer_vector(list(df_train['text cleaned'][i].split(' '))))\n",
        "##########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "680576d1"
      },
      "outputs": [],
      "source": [
        "##get test vectors##\n",
        "test_vectors=[]\n",
        "for i in range(len(df_test)):\n",
        "    test_vectors.append(model.infer_vector(list(df_test['text cleaned'][i].split(' '))))\n",
        "############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a6b46f4"
      },
      "outputs": [],
      "source": [
        "train=pd.DataFrame(train_vectors)\n",
        "train['folder name']=df_train['folder name']\n",
        "train['file name']=df_train['file name']\n",
        "test=pd.DataFrame(test_vectors)\n",
        "test['folder name']=df_test['folder name']\n",
        "test['file name']=df_test['file name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fde2eb19"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa1fdce6"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad25977f"
      },
      "outputs": [],
      "source": [
        "train_,validation=train_test_split( train, test_size=0.33, random_state=42) #split data into train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "650079c8"
      },
      "outputs": [],
      "source": [
        "len(train_),len(test),len(validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22b2aca1"
      },
      "outputs": [],
      "source": [
        "#save data sets\n",
        "train_.to_csv(proj_dir+'train_data.csv')\n",
        "test.to_csv(proj_dir+'test_data.csv')\n",
        "validation.to_csv(proj_dir+'validation_data.csv')"
      ]
    }
  ]
}