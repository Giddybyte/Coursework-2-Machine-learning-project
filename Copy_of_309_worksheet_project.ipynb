{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "zPEhSwTq7aCx"
      },
      "outputs": [],
      "source": [
        "from __future__ import division, print_function\n",
        "from gensim import models\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzXF7hOQ8N1b",
        "outputId": "0967ca22-e220-4004-b4f7-4c9c5277d089"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import initializers\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "U_NzlkIjvcjM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYMQ5ixq9Qi6",
        "outputId": "ac72e22f-a9f5-46ec-ad26-1617532a7662"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proj_dir='/content/drive/MyDrive/Colab Notebooks/doc2vec/' #give your project directory here. data sets should be in this location"
      ],
      "metadata": {
        "id": "s6VaBnuZ9Uca"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''this function is used to read text files.'''\n",
        "def read_txt_file(file_name):\n",
        "    with open(file_name,encoding=\"utf8\", errors='ignore') as f:\n",
        "        ###extract the boady of the text###\n",
        "        line = f.readline()\n",
        "        txt=''\n",
        "        txt=txt+' '+line\n",
        "        while line:\n",
        "            line = f.readline()\n",
        "            txt=txt+' '+line\n",
        "        ###################################\n",
        "    f.close()\n",
        "    return(txt)"
      ],
      "metadata": {
        "id": "mBJEDlXk9e4z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words('english'))#load stop words\n",
        "punctuations=string.punctuation #get punctuations\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "'''this function is used to clean text'''\n",
        "def clean_txt(txt):\n",
        "    txt=txt.lower() #set all characters to lowercase\n",
        "    sentences=txt.split('\\n')\n",
        "    txt = ' '.join([i for i in sentences if not ':' in i])#remove headers\n",
        "    txt = ''.join([i for i in txt if not i.isdigit()])#remove numbers\n",
        "\n",
        "    ###remove urls and emails###\n",
        "    words=txt.split()\n",
        "    txt = ' '.join([i for i in words if not '@' in i and not '.com' in i and not  'http:' in i])\n",
        "    #######################################\n",
        "\n",
        "    ###remove punctuations###\n",
        "    for character in punctuations:\n",
        "        txt = txt.replace(character, '')\n",
        "    #########################################\n",
        "    \n",
        "    ###remove stop words and lemmatize###\n",
        "    words=txt.split()\n",
        "    filtered_txt = ' '.join([lemmatizer.lemmatize(i) for i in words if not i in stop_words])\n",
        "    #####################################\n",
        "    \n",
        "    return(filtered_txt)\n"
      ],
      "metadata": {
        "id": "Sh72ccQC9mgw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_data(location):    \n",
        "    y=os.listdir(location)#get the list of folder\n",
        "    txts=[]\n",
        "    txts_cleaned=[]\n",
        "    folder_array=[]\n",
        "    file_array=[]\n",
        "    for i in range(len(y)):\n",
        "        text_file_names=os.listdir(location+'/'+y[i]) #get the list of files\n",
        "        for text_file_name in text_file_names:\n",
        "                file_array.append(text_file_name)\n",
        "                txt=read_txt_file(location+'/'+y[i]+'/'+text_file_name) #read the text file\n",
        "                txts.append(txt)\n",
        "                txts_cleaned.append(clean_txt(txt)) #clean the text\n",
        "                folder_array.append(y[i])\n",
        "\n",
        "    ###create a data frame###\n",
        "    df=pd.DataFrame()\n",
        "    df['texts']=txts\n",
        "    df['text cleaned']=txts_cleaned\n",
        "    df['folder name']=folder_array\n",
        "    df['file name']=file_array\n",
        "    ########################\n",
        "    return (df)\n"
      ],
      "metadata": {
        "id": "tCJi2iqF9nrR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=load_and_clean_data(proj_dir+'20news-bydate-train')\n",
        "df_test=load_and_clean_data(proj_dir+'20news-bydate-test')\n"
      ],
      "metadata": {
        "id": "cuEkdMOp96Xj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbqbrr7BEAW6",
        "outputId": "866ea143-6aa9-4e64-d9ef-565e719b23da"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PVO8pWNO5-fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION OF THE CLEANED TEXT"
      ],
      "metadata": {
        "id": "LS7lG2SM51pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "tokens = [word_tokenize(sen) for sen in df_train['text cleaned']]\n",
        "tokenss = [word_tokenize(sen) for sen in df_test['text cleaned']]\n"
      ],
      "metadata": {
        "id": "SqxOUSfYCb2v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATED ADDITION COLUMN FOR TOKENS\n"
      ],
      "metadata": {
        "id": "JEPbRrnl6BCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['tokens']= tokens\n",
        "df_test['tokenss']= tokenss\n"
      ],
      "metadata": {
        "id": "K6hhg4QIEs4r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(s.split()) for s in df_train['text cleaned']])\n",
        "print('Maximum length: %d' % max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzbR-zyM9_kX",
        "outputId": "9b0cbc57-09d8-47b8-8d1c-6091ec4d60c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length: 5890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1GRF4kFP7aC_"
      },
      "outputs": [],
      "source": [
        "data_train=df_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test=df_test"
      ],
      "metadata": {
        "id": "fn-DhevnnQwe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING THE VOCABULARY OF ALL WORDS"
      ],
      "metadata": {
        "id": "uwLoGcS968gM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vaugS-x7aC_",
        "outputId": "290f0360-1cc4-4cf9-8d2b-0c1f20a91b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1385315 words total, with a vocabulary size of 76895\n",
            "Max sentence length is 5892\n"
          ]
        }
      ],
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1qEp_1Wl7aC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d413b17-8972-42e7-f6a1-d72ee563436e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "894440 words total, with a vocabulary size of 57615\n",
            "Max sentence length is 5157\n"
          ]
        }
      ],
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokenss\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokenss\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in df_train['text cleaned']])\n",
        "print('Maximum length: %d' % max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFFMwKYAxgpJ",
        "outputId": "fad3fb9b-65e2-494d-9ddc-f9934a9d7e45"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length: 5890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 5890\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "ehC5mZMKxc7Z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UhIpcdM7aDA"
      },
      "source": [
        "### Tokenize and Pad sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgJo0W6I7aDA",
        "outputId": "a7694789-56b6-4d81-cde1-f01cf625ece5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 76897 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"text cleaned\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"text cleaned\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "G42gZJzY7aDB"
      },
      "outputs": [],
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"text cleaned\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_H5Gw2Mu7aDB"
      },
      "outputs": [],
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wty-rQi97aDA"
      },
      "source": [
        "### Get Embeddings-Using GLOVE word2vec\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "qMCfSy7FUImb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open('/content/glove.6B.300d.txt',encoding='utf8')# source of the downloaded glove files-use the 300d.txt\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors in Glove 6B 300d.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGSBCqdTnuq",
        "outputId": "6fa5bd1c-d103-47f1-8768-13f42bf0ac44"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 400000 word vectors in Glove 6B 300d.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.random.random((len(train_word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in train_word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "H8COVvd7VwBe"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(len(train_word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)"
      ],
      "metadata": {
        "id": "UoTAWPW7WLOL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predictor List**"
      ],
      "metadata": {
        "id": "sE6To8f9tZhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=df_train['folder name'].unique()\n",
        "label_names=list(class_names)\n"
      ],
      "metadata": {
        "id": "CqZa8tom0b7d"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=(np.unique(df_train['folder name'], return_inverse=True)[1])"
      ],
      "metadata": {
        "id": "jFq6YMaTO_Nc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train\n"
      ],
      "metadata": {
        "id": "bYv6qLBEPPHB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Neural Network without pre-training embeddings"
      ],
      "metadata": {
        "id": "OxSZNcvgwTJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "k0p80FQYRt0D"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(len(train_word_index)+1, EMBEDDING_DIM)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a multiclass unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(len(label_names), activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with multiclass crossentropy loss and an rmsprop optimizer.\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(\"Simple Neural Network\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNfOLgETP1s2",
        "outputId": "326c4bc1-a748-4fe1-f9c6-eb6f9871d4a8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Neural Network\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 5890)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 5890, 300)         23069400  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5890, 300)         0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1962, 128)         268928    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 652, 128)          114816    \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 128)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 20)                2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,472,236\n",
            "Trainable params: 23,472,236\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "hist=model.fit(x_train, y_tr, validation_split=0.2, epochs=epochs)"
      ],
      "metadata": {
        "id": "BJijFWFyR-68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Model LSTM-Simple(takes time to run, dont know why)"
      ],
      "metadata": {
        "id": "kbFPh4LWrBsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "l_lstm = Bidirectional(LSTM(5))(embedded_sequences)\n",
        "preds = Dense(len(label_names), activation='softmax')(l_lstm)\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"Bidirectional LSTM\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APMzednMWYE7",
        "outputId": "491619fc-2353-4471-f96f-1156597666a9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bidirectional LSTM\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 5890)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 5890, 300)         23069400  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 10)               12240     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                220       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,081,860\n",
            "Trainable params: 23,081,860\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training The Model**"
      ],
      "metadata": {
        "id": "psD1AjBFtutj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=30\n"
      ],
      "metadata": {
        "id": "_YBuBR-DarEz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cp=ModelCheckpoint('model_1.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
        "history=model.fit(x_train, y_tr, validation_split=0.2,epochs=epochs, batch_size=20)#,callbacks=[cp])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfup5yOFXnKg",
        "outputId": "b6b9dae6-9d87-4e4f-e631-30271d013965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "453/453 [==============================] - 396s 874ms/step - loss: 0.8493 - acc: 0.7627 - val_loss: 7.1346 - val_acc: 0.0601\n",
            "Epoch 2/30\n",
            "453/453 [==============================] - 395s 873ms/step - loss: 0.6914 - acc: 0.8084 - val_loss: 7.3120 - val_acc: 0.0742\n",
            "Epoch 3/30\n",
            "453/453 [==============================] - 396s 874ms/step - loss: 0.5756 - acc: 0.8360 - val_loss: 7.7305 - val_acc: 0.0959\n",
            "Epoch 4/30\n",
            "453/453 [==============================] - 398s 879ms/step - loss: 0.4943 - acc: 0.8548 - val_loss: 8.0169 - val_acc: 0.0893\n",
            "Epoch 5/30\n",
            "453/453 [==============================] - 398s 879ms/step - loss: 0.4298 - acc: 0.8745 - val_loss: 8.1832 - val_acc: 0.0937\n",
            "Epoch 6/30\n",
            "453/453 [==============================] - 398s 879ms/step - loss: 0.3813 - acc: 0.8876 - val_loss: 8.4028 - val_acc: 0.0972\n",
            "Epoch 7/30\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.3288 - acc: 0.9038Epoch 8/30\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.2966 - acc: 0.9191Epoch 9/30\n",
            "453/453 [==============================] - 397s 876ms/step - loss: 0.2399 - acc: 0.9486 - val_loss: 8.8892 - val_acc: 0.0924\n",
            "Epoch 10/30\n",
            "453/453 [==============================] - 398s 879ms/step - loss: 0.1851 - acc: 0.9680 - val_loss: 9.0955 - val_acc: 0.0924\n",
            "Epoch 11/30\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.1444 - acc: 0.9787Epoch 12/30\n",
            "453/453 [==============================] - 397s 877ms/step - loss: 0.1098 - acc: 0.9859 - val_loss: 9.4868 - val_acc: 0.0999\n",
            "Epoch 13/30\n",
            "453/453 [==============================] - 398s 878ms/step - loss: 0.0927 - acc: 0.9869 - val_loss: 9.7304 - val_acc: 0.0950\n",
            "Epoch 14/30\n",
            "453/453 [==============================] - 398s 878ms/step - loss: 0.0744 - acc: 0.9898 - val_loss: 10.0474 - val_acc: 0.0915\n",
            "Epoch 15/30\n",
            "453/453 [==============================] - 395s 873ms/step - loss: 0.0657 - acc: 0.9904 - val_loss: 10.2506 - val_acc: 0.0941\n",
            "Epoch 16/30\n",
            "453/453 [==============================] - 395s 872ms/step - loss: 0.0538 - acc: 0.9922 - val_loss: 10.5332 - val_acc: 0.0915\n",
            "Epoch 17/30\n",
            "453/453 [==============================] - 394s 871ms/step - loss: 0.0461 - acc: 0.9930 - val_loss: 10.6479 - val_acc: 0.0928\n",
            "Epoch 18/30\n",
            "453/453 [==============================] - 394s 869ms/step - loss: 0.0517 - acc: 0.9899 - val_loss: 10.9132 - val_acc: 0.0906\n",
            "Epoch 19/30\n",
            "453/453 [==============================] - 394s 870ms/step - loss: 0.0432 - acc: 0.9926 - val_loss: 11.1665 - val_acc: 0.0901\n",
            "Epoch 20/30\n",
            "453/453 [==============================] - ETA: 0s - loss: 0.0325 - acc: 0.9948Epoch 21/30\n",
            "122/453 [=======>......................] - ETA: 4:19 - loss: 0.0323 - acc: 0.9951"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second CNN MODEL- We need to discuss ( Runs fine By the validation and accuracy is not showing properly"
      ],
      "metadata": {
        "id": "Cf3mfnhO27Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def ConvNet1(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "    x = layers.MaxPooling1D(5)(x)\n",
        "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling1D(5)(x)\n",
        "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    preds = Dense(labels_index, activation='softmax')(x)\n",
        "    model = keras.Model(sequence_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "ob8Q2Ch6TaIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=df_train['folder name'].unique()\n",
        "label_names=list(class_names)\n",
        "len(label_names)\n"
      ],
      "metadata": {
        "id": "D1f07Ge7ZMDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=(np.unique(df_train['folder name'], return_inverse=True)[1])\n",
        "y_train"
      ],
      "metadata": {
        "id": "a2OkmMJj8UEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train\n",
        "\n"
      ],
      "metadata": {
        "id": "IQb2XTKdZSwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = ConvNet1(embedding_matrix, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                len(label_names))"
      ],
      "metadata": {
        "id": "lLLwQbYjZB5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "batch_size = 34"
      ],
      "metadata": {
        "id": "S8mddeVViyW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_train = np.random.rand(10,5890) #(batch_size, input_shape)\n",
        "y_tr = np.random.rand(10)\n",
        "y_tr=y_tr.astype(int)"
      ],
      "metadata": {
        "id": "wAUwD2sGsvw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training The second Model(CNN Model)"
      ],
      "metadata": {
        "id": "_AHRls1G2rnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model1.fit(x_train, y_tr, epochs=num_epochs, shuffle=True,validation_split=0.2, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-4mqAQWacgsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xXgz5hW7aDB"
      },
      "source": [
        "Another CNN Model with different Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IbyFuau7aDB"
      },
      "outputs": [],
      "source": [
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=500, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Flatten()(x) \n",
        "    preds = Dense(labels_index, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuhvjGPV7aDB"
      },
      "outputs": [],
      "source": [
        "# = data_train['folder name']\n",
        "class_names=df_train['folder name'].unique()\n",
        "label_names=list(class_names)\n",
        "len(label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZKmXjPH7aDC"
      },
      "outputs": [],
      "source": [
        "y_train=(np.unique(df_train['folder name'], return_inverse=True)[1])\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "79pzL1n1rH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cwLLw0D7aDC"
      },
      "outputs": [],
      "source": [
        "x_train = train_cnn_data\n",
        "y_train=(np.unique(df_train['folder name'], return_inverse=True)[1])\n",
        "y_tr=y_train\n",
        "y_tr=y_tr.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3yH76Ms7aDC"
      },
      "outputs": [],
      "source": [
        "model2 = ConvNet(embedding_matrix, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM,len(label_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv4dXL4e7aDC"
      },
      "source": [
        "Training the second CNN with Kernel model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZggdPNkT7aDC"
      },
      "outputs": [],
      "source": [
        "num_epochs = 30\n",
        "batch_size = 34"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_train = np.random.rand(10,5890) #(batch_size, input_shape)\n",
        "y_tr = np.random.rand(10)\n",
        "y_tr=y_tr.astype(int)"
      ],
      "metadata": {
        "id": "273givq7qzg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN9a4aF67aDC"
      },
      "outputs": [],
      "source": [
        "hist = model2.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.2, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8BgzDK_7aDC"
      },
      "source": [
        "Testing Part-Work in Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERZIh_Fz7aDC"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Copy of 309_worksheet_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}